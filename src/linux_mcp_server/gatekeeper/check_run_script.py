import json
import logging

from litellm import Choices
from litellm import completion
from litellm import get_supported_openai_params
from litellm import ModelResponse
from litellm import supports_response_schema
from pydantic import BaseModel

from linux_mcp_server.config import CONFIG
from linux_mcp_server.utils import StrEnum


logger = logging.getLogger("linux-mcp-server")


def get_model() -> str:
    if CONFIG.gatekeeper_model is None:
        raise ValueError("To use run_script tools, you must set gatekeeper_model in the linux-mcp-server config")
    else:
        return CONFIG.gatekeeper_model


READONLY_INSTRUCTION = """
 * The script does not modify the system in any way.
"""

READONLY_RESULT = """
 * `MODIFIES_SYSTEM`: The script modifies the system in some way.
"""

READONLY_USE_CASES = "get the status of a system, or diagnose a problem"
READWRITE_USE_CASES = "modify system settings, or fix a problem"

PROMPT = """\
The user is using a language model connected to the linux-mcp-server
Model Context Protocol server, and are using it to do one or more of the
following: {use_cases}.

You are a gatekeeper that checks if commands and scripts generated by the
language model are correct and correspond to the user's intent.

# General policies

 * Software can only be installed from pre-configured repositories.
   No new repositories may be added.
 * Except for installing software from pre-configured repositories,
   nothing may be downloaded from the internet.

# Your Task

In response to the user's prompt, the language model has generated the
following script of type {script_type}.

<START_OF_SCRIPT>
{script}
<END_OF_SCRIPT>

and has described what the script does with the following text:

<START_OF_DESCRIPTION>
{description}
<END_OF_DESCRIPTION>

Your job is to check that:

{readonly_instruction}* The script does what is described, and does not do anything unrelated.
* It conforms to the general policies outlined above.
* The script is clear and simple and written in an obvious, expected
  manner.
* The script does not contain malicious code or introduce security vulnerabities
  on the system.

It is *OK* if the script does not consider every possible failure mode, as long
as it fails cleanly. The model may have previously investigated the system
and found out things like the operating system version and configuration
file paths.

Return a JSON object with the following keys, `status` (required) and `detail` (optional).

status should be one of the following:

* `OK`: The script does what is described and safe to run.
* `BAD_DESCRIPTION`: The script does not do what is described.
* `POLICY`: The script violates one or more of the general policies outlined above.
{readonly_result}* `UNCLEAR`: The script is unclear or difficult to understand.
* `DANGEROUS`: The script is likely to cause damage to the system
* `MALICIOUS`: The script contains malicious code or would introduce security vulnerabilities.

If the status is not OK, the detail should be short 1-3 sentence description of
what is wrong with the script. Be specific to allow the language model to correct
the problem.

If the script seems buggy but does not fall into any of the categories above, return
a status of `OK`.

Example response (respond with only the JSON object - do not wrap it in a code block).
{{ "status": "DANGEROUS", "detail": "The critical example file `/etc/example.conf` will be deleted if an error occurs in the first call to sed." }}
"""


class GatekeeperStatus(StrEnum):
    OK = "OK"
    BAD_DESCRIPTION = "BAD_DESCRIPTION"
    POLICY = "POLICY"
    MODIFIES_SYSTEM = "MODIFIES_SYSTEM"
    UNCLEAR = "UNCLEAR"
    DANGEROUS = "DANGEROUS"
    MALICIOUS = "MALICIOUS"


class GatekeeperResult(BaseModel):
    status: GatekeeperStatus
    detail: str


def check_run_script(description: str, script_type: str, script: str, *, readonly: bool) -> GatekeeperResult:
    # Check that the script does what is described
    if "end_of_script" in script.lower():
        return GatekeeperResult(status=GatekeeperStatus.MALICIOUS, detail="Script contains 'end_of_script'")

    prompt = PROMPT.format(
        script_type=script_type,
        script=script,
        description=description,
        use_cases=READONLY_USE_CASES if readonly else READWRITE_USE_CASES,
        readonly_instruction=READONLY_INSTRUCTION if readonly else "",
        readonly_result=READONLY_RESULT if readonly else "",
    )

    messages = [{"role": "user", "content": prompt}]

    params = get_supported_openai_params(model=get_model())
    if params is not None and "response_format" in params:
        if supports_response_schema(model=get_model()):
            response_format = GatekeeperResult
        else:
            logger.warning(
                f"{get_model()} supports 'response_format' parameter, but does not support response schema. Falling back to text response.",
                extra={"model": get_model(), "params": params},
            )
            response_format = None
    else:
        response_format = None

    response = completion(model=get_model(), messages=messages, response_format=response_format)
    assert isinstance(response, ModelResponse)
    assert isinstance(response.choices[0], Choices)
    response_text = (response.choices[0].message.content or "").strip()

    logger.info(f"Gatekeeper response: {response_text}")

    if response_format is not None:
        return GatekeeperResult.model_validate_json(response_text)

    try:
        response_data = json.loads(response_text)
    except json.JSONDecodeError:
        raise RuntimeError("Failed to parse response from gatekeeper model")

    if not isinstance(response_data, dict):
        raise RuntimeError("Invalid response format from gatekeeper model")

    try:
        status = GatekeeperStatus(response_data.get("status", ""))
    except ValueError:
        raise RuntimeError("Bad status in gatekeeper model response")

    detail = response_data.get("detail", "")

    return GatekeeperResult(status=status, detail=detail)
